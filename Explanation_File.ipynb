{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Engin X data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Engin X is an instrument that uses neutron spectroscopy to find various characteristics of a material most notably the strain/stress. Over the years as scientists have used Engin X a large backlog of data has accumulated, with inconsistencies; format, variable names, ect. This is an attempted to “Clean the data” by formatting it in a clearer and more consistent fashion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly the tools I have used are python with various libraries; os, numpy, math, xlsxwriter, pandas for most of the data manipulation, threading to speed up the code as the data set was quite large and I had a server to work with and mantid and ISIS specific module that gave me access to data that was in .raw and .nsx files types. \n",
    "\n",
    "I define “Cleaning the data” as moving it from the format it was in: txt files titled with a variable name and unique run number and consisted of a column of time and a column of a measured variable, these files were separated by their cycles, all together Engin X has ran for about 78 cycles. I moved all this data to a new format focusing on it being pandas friendly, the data is now in csv files for each cycle with all the data for each cycle in one large csv file that can be pulled into a pandas dataframe using pd.read_csv(using chunksize is recommended). Data from the neutron detector is in a separate file that can be accessed using the keys in the large csv file and a function I have made. This all in all has made the data far easier to work with and sort through using pandas’s pandas.dataframe.groupby() function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "I “Cleaned the data” in steps as it makes it easier to understand and allowed me to compartmentalise the problems. I go over Threading as the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import xlsxwriter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import isnull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier the data in the .raw and .nsx files were extracted using Mantid, they both have the same data contained within them, however .nsx files were only recently used so I extracted all the data using .raw files as it was more universally used. Mantid hasn’t been fully updated to Python 3 yet, to get around this I used Mantid for python 2.7 for the first bit of this project. I extracted 7 pieces of data from the raw files, the X and Y data from the North and South detectors, neutron beam start time, neutron beam end time, title of the run. I then took this data and put it in a more accessible file type, txt with a separator string that never shows up in the data so I could easily split up the data later using python’s split method on strings. This is all done in the file titled Mantid in the repository.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the code I use many functions to manipulate the original data as the indexing changed over time and I needed to do things like find the delimiter of the run number or the index of all the runs in a given cycle. Thus I made the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)\n",
    "    return directory\n",
    "\n",
    "def FindDel(S):\n",
    "    \"\"\"This function takes a the first string in a list,(S) that starts with ENG, it finds the nomancature \n",
    "    the file uses for the begining of each file, e.g ENG44007 or ENGINX00305258, it shall also work if more numbers\n",
    "    are added or the name changes so long as ENG followed by anything other than \".\" or \"_\" or \" \" is still \n",
    "    the start of each file\"\"\"\n",
    "    for i in S:\n",
    "        if i[0:3] == 'ENG':\n",
    "            n = 1\n",
    "            for j in i:\n",
    "                if j == '.':\n",
    "                    break\n",
    "                elif j == ' ':\n",
    "                    break\n",
    "                elif j == '_':\n",
    "                    break\n",
    "                else: \n",
    "                    n += 1 \n",
    "            return n\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "def InitialRun(S):\n",
    "    \"\"\"This takes S a list of filenames and returns the first run index\"\"\"\n",
    "    last_int_index = FindDel(S) -1\n",
    "    first_int_index = 0 \n",
    "    for i in S:                                                          \n",
    "        if i[0:3] == 'ENG':\n",
    "            for j in range(last_int_index):\n",
    "                try:\n",
    "                    int(i[j])\n",
    "                    run_number = i[first_int_index:last_int_index]\n",
    "                    break\n",
    "                except:\n",
    "                    first_int_index += 1\n",
    "        else:\n",
    "            continue\n",
    "        break\n",
    "    return((run_number))\n",
    "\n",
    "def CycleRuns(S):\n",
    "    \"\"\"This takes a list of filenames and returns a list of all the runs in the file\"\"\"\n",
    "    Start_run = int(InitialRun(S))\n",
    "    Last_run = int(InitialRun(reversed(S)))\n",
    "    Runs = range(Start_run,Last_run)\n",
    "    return (list(Runs))\n",
    "\n",
    "def RunIndex(s,n): #ENGINX00305258\n",
    "    \"\"\"s is a string in the for m of a filename from the enginx data and returns it's run number, n is the FindDel\"\"\"\n",
    "    last_int_index = n-1\n",
    "    first_int_index = 0\n",
    "    for j in range(last_int_index):\n",
    "        try:\n",
    "            int(s[j])\n",
    "            return s[first_int_index:last_int_index]\n",
    "        except:\n",
    "            first_int_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all the same variable in the data over time has been given many different names I needed a way to find a way to identify each variable to a single name. I chose that this would be best done with a dictionary that takes every given variable name(Names that are given in the uncleaned data) as keys and returns the universal variable name(Names that I have given to the variables) as values. The universal names are also used as the headers for the csv files once the data is cleaned. \n",
    "\n",
    "This was done in two parts first I used the code below to make a spreadsheet of all the unique non temporary file names and the cycle that file was found. Temporary files have data that are in the .raw files, they are created so scientists can check how the data is coming along mid experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"D:\\\\\" # Directory where data is being stored\n",
    "array_names = []\n",
    "file_years = []\n",
    "long_array_names = []\n",
    "for foldername in os.listdir(directory):\n",
    "    if foldername[0:5] == 'cycle':\n",
    "        n = FindDel(os.listdir(\"D:\\\\\"+foldername))\n",
    "        print(n)\n",
    "        for filename in os.listdir(\"D:\\\\\"+foldername):\n",
    "            if filename[n:] in array_names: #stop repeated file names from entering \n",
    "                pass\n",
    "            elif len(filename) < n+1: # Stop useless file names from entering (e.g copy)\n",
    "                pass\n",
    "            elif filename[n] in (\"S\", \"s\", \"N\", \"n\"):# Stop temporary files from entering (e.g .s01 or .n001)\n",
    "                try:\n",
    "                    int(filename[n+1]) # all temp files are 's' or 'n' followed by a int\n",
    "                    temp_files.append(filename)\n",
    "                except ValueError: \n",
    "                    array_names.append(filename[n:])\n",
    "                    long_array_names.append(filename)\n",
    "                    file_years.append(foldername)\n",
    "            else: # most unique file names go here \n",
    "                array_names.append(filename[n:])\n",
    "                long_array_names.append(filename)\n",
    "                file_years.append(foldername)\n",
    "                \n",
    "#print(file_years)\n",
    "#print(array_names)\n",
    "#print(long_array_names)\n",
    "\n",
    "workbook   = xlsxwriter.Workbook('Filenames.xlsx')\n",
    "\n",
    "worksheet1 = workbook.add_worksheet()\n",
    "worksheet2 = workbook.add_worksheet()\n",
    "\n",
    "worksheet1.write_column('A1', array_names)\n",
    "worksheet2.write_column('A1', long_array_names)\n",
    "worksheet1.write_column('B1', file_years)\n",
    "worksheet2.write_column('B1', file_years)\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here the only way I thought of writing in the universal names is by manually typing them into the spreadsheet, which is what I did. I made a function that would take the spreadsheet and turn it into a dictionary and give a list of the column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\zahaab\\\\ENGIN_X_DATA_CLEANING\\\\File_names_2013-19.xlsx\" # The spreadsheet of file names\n",
    "def Translator(path):\n",
    "    \"\"\"This function takes the path to a spreadsheet and makes a dictionary between two columns and returns the \n",
    "    universal file names as a list, I use it to make a dictionary to translate the given file names to the universal \n",
    "    names\"\"\"\n",
    "    xls = pd.ExcelFile(path)\n",
    "    df = xls.parse(sheet_name = \"Sheet1\")\n",
    "    df = df[[\"Filename\", \"Column_name\"]]\n",
    "    df = df.fillna(0)\n",
    "    true_name_translater = dict(zip(df[\"Filename\"],df[\"Column_name\"]))\n",
    "    s = set(true_name_translater.values())\n",
    "    col_names = list(s)\n",
    "    col_names.remove(0)\n",
    "    col_names.append(\"measured_temperature_U\")#This is used later as some temperature mesurments don't \n",
    "    #note the number of the input used\n",
    "    col_names.append(\"unregistered_value\") #A place for values that aren't specified\n",
    "    return true_name_translater, col_names\n",
    "\n",
    "true_name_translater, col_names = Translator(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the largest step and it involves extracting all the data from the txt files and the .raw files and then putting those into a new format. These are now in directories with the name of the cycle, those directories consists of txt files, named after the unique run number of the data that it contains, the txt files are in csv format with all the data from that run in one place. This is done using 4 functions that I’ll explain one at a time. \n",
    "\n",
    "There are 3 data types being passed back and forth throughout this step:\n",
    "\n",
    "raw_data: dictionary of the key’s being the from the strings from the neutron data list shown later and the values being the data itself.\n",
    "\n",
    "time_list: is a numpy array of all the time values in a given run\n",
    "\n",
    "Run_data: this is the data from all the .txt values held in a list of two element tuples; element 1 is the universal variable name, element 2 is a dictionary of time\\date keys and float values of the variable being measured at that time. Reasoning for this is that it’s faster than a dictionary in a dictionary and lets the NewRun function be far more concise than other data structures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddRawData(run_number):\n",
    "    \"\"\"Takes the run number of a raw file, this returns  a dict of the .raw data that came from that raw file as a \n",
    "    dictionary of the key’s being the from the strings from the neutron data list shown later and the values being \n",
    "    the data itself. However for the X and Y values for the North and South detectors it inputs a string into the \n",
    "    dictionary and makes a file in a different directory with the data, this string can be used \n",
    "    with the NeutronDataTranslator function to get the Neutron data.\"\"\"\n",
    "    file_path = output+\"\\\\\"+str(run_number)+\".txt\"\n",
    "    output_file = open(file_path, \"r\")\n",
    "    contents = output_file.read()\n",
    "    split_contents = contents.split(\"kamehameha\") # Had to pick somthing that wouldn't be in data\n",
    "    del split_contents[-1]\n",
    "    clean_split_contents = []\n",
    "    for i,j in zip(split_contents, data):\n",
    "        a = i.replace(\"\\n\",\"\").replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "        a = a.split(\" \")\n",
    "        a = list(filter(None, a))\n",
    "        if \"data\" in j:\n",
    "            clean_split_contents.append(j[0:16]+str(run_number))\n",
    "            Neutron_file = open(Neutron_folder+\"\\\\\"+j[0:16]+str(run_number)+\".txt\", \"w+\")\n",
    "            Neutron_file.write(str(a))\n",
    "            Neutron_file.close()\n",
    "        elif \"time\" in j:\n",
    "            untdate = i.replace('T',' ')\n",
    "            clean_split_contents.append(untdate)\n",
    "        else:\n",
    "            clean_split_contents.append(a)\n",
    "    raw_data = dict(zip(data, list(clean_split_contents)))\n",
    "    output_file.close()\n",
    "    os.remove(file_path) \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in step one the .raw data is now in txt files with a string to split on. The AddRawData function opens the txt file with a certain run’s raw data splits it as to get the different data seperated. An issue you will notice that comes up a lot is that some of the time values in the data have a “T” in the center and some don’t, this is often solved in the data with an if statement which checks if there is a “T” and if one is found removes it and reformats the time. As mentioned in the doc string some data isn’t in the dictionary or the final csv file to save space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeutronDataTranslator(Neutron_key):\n",
    "    \"Takes a Neutron_key and returns the list of data associated with it. \"\n",
    "    Neutron_file = open(Neutron_folder+\"\\\\\"+Neutron_key+\".txt\", \"r\")\n",
    "    contents = Neutron_file.read()\n",
    "    contents = contents.replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "    contents = contents.split(\", \")\n",
    "    contents = list(map(float, contents))\n",
    "    Neutron_file.close()\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NeutronDataTranslator does as the doc string says. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NewRun(run_number, run_data, time_list, raw_data, file_location):\n",
    "    \"\"\"This activates if a new run has started, it makes a new list of dictionaries and makes a CSV out of current data\"\"\"\n",
    "    if type(time_list) == int:\n",
    "        df1 = pd.DataFrame()\n",
    "        df1[\"Date/Time\"] = raw_data[\"neutron_start_time\"]\n",
    "        for i in raw_data:\n",
    "            df1[str(i)] = str(raw_data[i])\n",
    "        print(run_number + \" in \" + file_location)\n",
    "        df1.to_csv(file_location+\"\\\\\"+run_number)\n",
    "    else:\n",
    "        df1 = pd.DataFrame()\n",
    "        for i in time_list:\n",
    "            row = np.array([i])\n",
    "            column_names = np.array([\"Date/Time\"])\n",
    "            for j in run_data:\n",
    "                try:\n",
    "                    row = np.append(row, [j[1][i]])\n",
    "                    column_names = np.append(column_names, [j[0]])\n",
    "                except:\n",
    "                    pass\n",
    "            try:\n",
    "                df2 = pd.DataFrame([row], columns = column_names)\n",
    "                df1 = df1.append(df2, sort=False)\n",
    "            except:\n",
    "                df2 = pd.DataFrame([row][:-1], columns = column_names)\n",
    "                df1 = df1.append(df2, sort=False)\n",
    "        for i in raw_data:\n",
    "            df1[str(i)] = str(raw_data[i])\n",
    "        print(run_number + \" in \" + file_location)\n",
    "        df1.to_csv(file_location+\"\\\\\"+run_number)\n",
    "    \n",
    "    raw_data = {}\n",
    "    run_data = []\n",
    "    time_list = np.array([])\n",
    "    for i,j in enumerate(col_names):\n",
    "        run_data.append((j,{}))\n",
    "    \n",
    "    return run_data,time_list,raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NewRun is the function that resets all the locally remembered variables for each run and makes the file for the run that has just finished being processed. It does this by making a pandas data frame that is empty apart from the column name of Date/Time, from there data from the txt files are loaded into a row of data, put into a second data frame, the second data frame is appended to the first and this iterates until all the run data is in one data frame. The raw data is then added to the data frame as it is constant a column is initiated with the  default value from the raw data values and column names from the raw data keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddData(file_path, filename, n, run_data, time_list):\n",
    "    \"\"\"This takes data from a txt file and puts it into a list of times and the dictionaries\"\"\"\n",
    "    df = pd.read_csv(file_path, sep = \"\\t\", names = [\"Date/Time\", str(true_name_translater[filename[n:]])])\n",
    "    file_times = df[\"Date/Time\"].values\n",
    "    file_data = df[str(true_name_translater[filename[n:]])].values \n",
    "    dict_index = Find_dict(run_data, true_name_translater[filename[n:]])\n",
    "    if os.stat(file_path).st_size == 0: # remove empty files\n",
    "        pass\n",
    "    elif len(str(df.iloc[0].values[0])) < 18: # remove the files without time in the first colmn\n",
    "        pass\n",
    "    elif file_times[0][10] ==\"T\":\n",
    "        for i in range(len(li)): # fix the T issue in some times \n",
    "            untdate = file_times[0].replace('T',' ')\n",
    "            file_times = np.append(file_times,[untdate])\n",
    "            file_times = np.delete(file_times,0, axis = None)\n",
    "        time_list = np.unique(np.append(time_list, file_times))\n",
    "        for i, j in zip(file_times,file_data):\n",
    "            if \"temperature\" in true_name_translater[filename[n:]]:#Alot of tempreture files, may be some overlap in time\n",
    "                try:\n",
    "                    a = run_data[dict_index][1][i]\n",
    "                    run_data[-2][1][i] = j\n",
    "                except:\n",
    "                    run_data[dict_index][1][i] = j\n",
    "            else:#rest of the files \n",
    "                try:\n",
    "                    a = run_data[dict_index][1][i]\n",
    "                    run_data[-1][1][i] = [run_data[dict_index][0],j]\n",
    "                except:\n",
    "                    run_data[dict_index][1][i] = j\n",
    "    else: # Normal time columns\n",
    "        time_list = np.unique(np.append(time_list, file_times))\n",
    "        for i, j in zip(file_times,file_data):\n",
    "            if \"temperature\" in true_name_translater[filename[n:]]:\n",
    "                try:\n",
    "                    a = run_data[dict_index][1][i]\n",
    "                    run_data[-2][1][i] = j\n",
    "                except:\n",
    "                    run_data[dict_index][1][i] = j\n",
    "            else:\n",
    "                try:\n",
    "                    a = run_data[dict_index][1][i]\n",
    "                    run_data[-1][1][i] = [run_data[dict_index][0],j]\n",
    "                except:\n",
    "                    run_data[dict_index][1][i] = j\n",
    "    return run_data, time_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AddData is the function that takes the data from the txt file and puts it into run_data, it uses a lot of the things made and explained previously. It also uses another function Find_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_dict(run_data, trans_filename):\n",
    "    \"\"\"Takes the run_data dictionary and the universal file name (called trans_filename as true name translator \n",
    "    must be used) and returns the index the universal file name occurs in the run_data. \"\"\"\n",
    "    for i,j in enumerate(run_data):\n",
    "        if j[0] == trans_filename:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does what the doc string says."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "#%prun\n",
    "directory = r\"C:\\Users\\zahaab\\test_case4\"\n",
    "output = directory + \"\\\\temp_raw_data\"\n",
    "\n",
    "Neutron_folder = CreateFolder(directory+\"\\\\\"+\"Neutron_data\")\n",
    "ENGINX_DATA = CreateFolder(directory+\"\\\\\"+\"clean_data\")\n",
    "data = [\"North_neutron_X_data\",\"North_neutron_Y_data\",\"South_neutron_X_data\",\n",
    "            \"South_neutron_Y_data\",\"neutron_start_time\",\"neutron_end_time\",\"Run_Title\"]\n",
    "\n",
    "for foldername in os.listdir(directory):\n",
    "    print(foldername)\n",
    "    if foldername[0:5] == 'cycle':\n",
    "        folder_path = directory+\"\\\\\"+foldername\n",
    "        clean_data = CreateFolder(ENGINX_DATA+\"\\\\\"+foldername)\n",
    "        n = FindDel(os.listdir(folder_path))\n",
    "        filenames = os.listdir(folder_path)\n",
    "        raw_data = {}\n",
    "        run_data = []\n",
    "        for i,j in enumerate(col_names):\n",
    "            run_data.append((j,{}))\n",
    "        time_list = np.array([])\n",
    "        finale_file = 0\n",
    "        run_index = RunIndex(filenames[0],n)\n",
    "        \n",
    "        for filename in reversed(filenames):  \n",
    "            if filename[0:3] == 'ENG':\n",
    "                finale_file = filename\n",
    "                break\n",
    "        \n",
    "        for k, filename in enumerate(filenames):\n",
    "            print(filename)\n",
    "            if k == 0:\n",
    "                continue\n",
    "                \n",
    "            elif \"Copy\" in filename:\n",
    "                continue\n",
    "                \n",
    "            elif \"copy\" in filename:\n",
    "                continue\n",
    "                \n",
    "            if filename[n] in (\"S\", \"s\", \"N\", \"n\"):# Stop temporary files from entering (e.g .s01 or .n001)\n",
    "                try:\n",
    "                    int(filename[n+1]) # all temp files are 's' or 'n' followed by a int\n",
    "                    continue\n",
    "                except ValueError: \n",
    "                    pass\n",
    "            \n",
    "            print(RunIndex(filenames[k],n))\n",
    "            print(filename[-3:])\n",
    "            \n",
    "            file_path = folder_path+'\\\\'+filename\n",
    "            if true_name_translater[filename[n:]] == 0:\n",
    "                pass\n",
    "                \n",
    "            elif filename[-3:] == \"nxs\":\n",
    "                pass\n",
    "            \n",
    "            elif RunIndex(filenames[k],n) != run_index:\n",
    "                run_index = RunIndex(filenames[k],n)\n",
    "                if len(time_list) == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    run_data,time_list,raw_data = NewRun(RunIndex(filenames[k-1],n), run_data, time_list, raw_data, clean_data)\n",
    "                    if filename[-3:].lower() in [\"RAW\", \"raw\", \"Raw\"]:\n",
    "                        raw_data = AddRawData(RunIndex(filenames[k],n))\n",
    "                    elif filename[-3:] == \"txt\":\n",
    "                        run_data,time_list = AddData(file_path, filename, n, run_data, time_list)\n",
    "                    else:\n",
    "                        pass\n",
    "            else:\n",
    "                if filename[-3:].lower() in [\"RAW\", \"raw\", \"Raw\"]:\n",
    "                    raw_data = AddRawData(RunIndex(filenames[k],n))\n",
    "                elif filename[-3:] == \"txt\":\n",
    "                    run_data,time_list = AddData(file_path, filename, n, run_data, time_list)\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            if filename == finale_file:\n",
    "                break\n",
    "        NewRun(RunIndex(finale_file,n), run_data, time_list, raw_data, clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we must run use the functions and this loop finds when each function must be used and uses it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This step is the most cpu intensive as it involves taking all the data from the newly made directories for each cycle and combines them with all the data from the stress rig for that cycle using the pandas concat function with two large pandas data frames.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def CombineRigCleanData(cycle):\n",
    "    direct = \"E:\\\\stuff\\\\OtherData\\\\\"+cycle+\"\\\\Stress Rig\"\n",
    "    \n",
    "    df1 = pd.DataFrame()\n",
    "    for filename in os.listdir(ENGINX_DATA+\"\\\\\"+cycle):\n",
    "        file_path = ENGINX_DATA+\"\\\\\"+cycle+\"\\\\\"+filename\n",
    "        df2 = pd.read_csv(file_path,index_col=0)\n",
    "        df2[\"Run_Number\"] = filename\n",
    "        df1 = df1.append(df2, ignore_index=False, sort=False)\n",
    "    time_list = df1[\"Date/Time\"].values\n",
    "    df1.set_index(\"Date/Time\") \n",
    "\n",
    "    df3 = pd.DataFrame()\n",
    "    for filename in os.listdir(direct):\n",
    "        file_path = direct+\"\\\\\"+filename\n",
    "        df4 = pd.read_csv(file_path, sep = \"|\", skipinitialspace = True, skiprows = [0,1])\n",
    "        df3 = df3.append(df4, ignore_index=False, sort=False)\n",
    "        input_file = open(file_path+\".txt\", \"r\")\n",
    "        first_line = output_file.readline()\n",
    "        constants = {\"Cross_Sectional_Area\" : first_line[:33].split(\"=\")[-1].replace(\" \", \"\"),\n",
    "                     \"Gauge_Length_for_Strain1\" : first_line[33:].split(\"=\")[-1].replace(\" \", \"\")}\n",
    "        input_file.close()\n",
    "        for i in constants:\n",
    "            df3[str(i)] = str(constants[i]).replace(\"\\n\",\"\")\n",
    "\n",
    "    df3 = df3.rename(index = str, columns = dict(zip(\n",
    "        df3.columns.values.tolist(), ['rig_{0}'.format(i).replace(\" \", \"\") for i in df3.columns.values.tolist()])))\n",
    "    df3 = df3.rename(index = str, columns = {'rig_Date/Time' : 'Date/Time'})\n",
    "\n",
    "    #df3.set_index(\"Date/Time\")\n",
    "    #df3 = df3.loc[~df3.index.duplicated(keep='first')] #drop duplicates \n",
    "    df3 = df3.loc[~df3[\"Date/Time\"].duplicated(keep='first')]\n",
    "    df5 = df3.loc[df3[\"Date/Time\"].isin(time_list)]\n",
    "    result = pd.concat([df5, df1], axis=1, sort=False, join='outer')\n",
    "\n",
    "    return(result) \n",
    "\n",
    "def CleanInstron01(cycle, filenames, clean_file_time):\n",
    "    direct = r\"E:\\stuff\\OtherData\\INSTRON_01\"\n",
    "    \n",
    "    df1 = pd.DataFrame()\n",
    "    for filename in os.listdir(ENGINX_DATA+\"\\\\\"+cycle):\n",
    "        file_path = ENGINX_DATA+\"\\\\\"+cycle+\"\\\\\"+filename\n",
    "        df2 = pd.read_csv(file_path,index_col=0)\n",
    "        df2[\"Run_Number\"] = filename\n",
    "        df1 = df1.append(df2, ignore_index=False, sort=False)\n",
    "    time_list = df1[\"Date/Time\"].values\n",
    "    df1.set_index(\"Date/Time\") \n",
    "    \n",
    "    #find the files that needed for the spesific cycle\n",
    "    cycle_start, cycle_end = time_list[0], time_list[-1]\n",
    "    start_file_index, end_file_index = 0, 0\n",
    "    for i in clean_file_time:\n",
    "        if datetime.strptime(i, \"%Y-%m-%d %H:%M:%S\") > datetime.strptime(cycle_start, \"%Y-%m-%d %H:%M:%S\"):\n",
    "            start_file_index = clean_file_time.index(i)-1\n",
    "            break \n",
    "    for i in clean_file_time.reverse():\n",
    "        if datetime.strptime(i, \"%Y-%m-%d %H:%M:%S\") < datetime.strptime(cycle_start, \"%Y-%m-%d %H:%M:%S\")::\n",
    "            end_file_index = (clean_file_time.index(i)+1)*-1\n",
    "            break \n",
    "    \n",
    "    if start_file_index, end_file_index == 0,0:#Logic needs to be fixed\n",
    "        return(df1)\n",
    "    \n",
    "    df3 = pd.DataFrame()\n",
    "    for filename in os.listdir(direct)[start_file_index, end_file_index+1]:\n",
    "        file_path = direct+\"\\\\\"+filename\n",
    "        df4 = pd.read_csv(file_path, sep = \"\\t\", skipinitialspace = True, skiprows = [0,1,2,3])\n",
    "        df3 = df3.append(df4, ignore_index=False, sort=False)\n",
    "        input_file = open(file_path+\".txt\", \"r\")\n",
    "        first_line, second_line, third_line = output_file.readline(), output_file.readline(1), output_file.readline(2)\n",
    "        constants = {\"Cross_Sectional_Area\" : first_line.split(\"=\")[-1].replace(\" \", \"\"),\n",
    "                     \"Gauge_Length_for_Strain1\" : second_line.split(\"=\")[-1].replace(\" \", \"\"),\n",
    "                     \"RB Number\" : third_line.split(\"=\")[-1].replace(\" \", \"\")}\n",
    "        input_file.close()\n",
    "        for i in constants:\n",
    "            df3[str(i)] = str(constants[i]).replace(\"\\n\",\"\")\n",
    "        \n",
    "    df3 = df3.rename(index = str, columns = dict(zip(\n",
    "        df3.columns.values.tolist(), ['rig_{0}'.format(i).replace(\" \", \"\") for i in df3.columns.values.tolist()])))\n",
    "    df3 = df3.rename(index = str, columns = {'rig_Date/Time' : 'Date/Time'})\n",
    "    df3 = df3.loc[~df3[\"Date/Time\"].duplicated(keep='first')]\n",
    "    \n",
    "    unttime = []\n",
    "    for i in df3[\"Date/Time\"].values:\n",
    "        untdate = \"{}-{}-{} {}:{}:{}\".format(\n",
    "            date[0:4], date[5:7], date[8:10], date[11:13], date[14:16], date[17:19])\n",
    "        unttime.append(untdate)\n",
    "    df3 = df3.drop(columns=[\"Date/Time\"])\n",
    "    df3[\"Date/Time\"] = unttime\n",
    "    \n",
    "    df5 = df3.loc[df3[\"Date/Time\"].isin(time_list)]\n",
    "    result = pd.concat([df5, df1], axis=1, sort=False, join='outer')\n",
    "    return(result) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two functions do the same thing for the most part, the method for storing the stress rig data has changed over time thus both of these functions are needed, they do as the description of step 5 says. Of Course they do a lot of formatting of the data as to make sure to keep all the nomenclature consistent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGINX_DATA = r\"E:\\clean_data\"\n",
    "\n",
    "instron_start = len(os.listdir(ENGINX_DATA))\n",
    "\n",
    "filenames = os.listdir(r\"E:\\stuff\\OtherData\\INSTRON_01\")\n",
    "clean_file_time = []\n",
    "for filename in filenames:\n",
    "    if \"continuous\" in filename:\n",
    "        filenames.remove(filename)\n",
    "    else: \n",
    "        date = filename[11:]\n",
    "        untdate = \"{}-{}-{} {}:{}:{}\".format(\n",
    "        date[0:4], date[5:7], date[8:10], date[11:13], date[14:16], date[17:19])\n",
    "        clean_file_time.append(untdate)\n",
    "            \n",
    "for i,j in  enumerate(os.listdir(ENGINX_DATA)):\n",
    "    if j == \"cycle_17_3\":\n",
    "        CombineRigCleanData(j)\n",
    "        instron_start = i\n",
    "    elif i > instron_start:\n",
    "        CleanInstron01(j, filenames, clean_file_time)\n",
    "    else:\n",
    "        CombineRigCleanData(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have to make a list of filenames that are in the instron directory as we must identify the cycle; the cycle isn’t mentioned in the instron data. This is used along with the clean file times to identify the cycle for CleanInstron01.\n",
    "\n",
    "Finally we must run use the functions and this loop finds when each function must be used and uses it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
